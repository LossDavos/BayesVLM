{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP with MLP Head and Dropout Probabilistic Inference\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Add a trainable MLP layer to a frozen CLIP model\n",
    "2. Train the MLP on a classification task\n",
    "3. Use dropout for probabilistic inference and uncertainty estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') # add bayesvlm to path\n",
    "sys.path.append('../models') # add models to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative import order to avoid circular imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Import torch modules\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import torchvision first to avoid circular imports\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Import torchmetrics after torchvision is fully loaded\n",
    "try:\n",
    "    from torchmetrics.classification import MulticlassCalibrationError, MulticlassAccuracy\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing torchmetrics: {e}\")\n",
    "    print(\"Try: pip install torchmetrics\")\n",
    "\n",
    "# Import custom modules\n",
    "from clip_mlp import CLIPWithMLP\n",
    "from bayesvlm.data.factory import DataModuleFactory\n",
    "from bayesvlm.utils import get_transform\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "import torch\n",
    "\n",
    "dataset = 'cifar10'  # or 'food101', 'cifar100', etc.\n",
    "clip_model_name = \"clip-base\"  # Use bayesvlm naming: 'clip-base', 'clip-large', 'clip-huge'\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "mlp_hidden_dim = 512\n",
    "mlp_dropout_rate = 0.3\n",
    "mlp_num_layers = 3\n",
    "n_uncertainty_samples = 100\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: cifar10\n",
      "Number of classes: 10\n",
      "Train samples: 40000\n",
      "Test samples: 10000\n",
      "\n",
      "Number of classes: 10\n",
      "Train samples: 40000\n",
      "Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# Create data module\n",
    "transform = get_transform('clip', 224)  # CLIP input size\n",
    "\n",
    "f = DataModuleFactory(\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    train_transform=transform,\n",
    "    test_transform=transform,\n",
    "    shuffle_train=True,\n",
    ")\n",
    "dm = f.create(dataset)\n",
    "dm.setup()\n",
    "\n",
    "# Get number of classes\n",
    "num_classes = len(dm.class_prompts)\n",
    "print(f\"Dataset: {dataset}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Train samples: {len(dm.train_ds)}\")\n",
    "print(f\"Test samples: {len(dm.test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/Desktop/DAP/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 151,807,756\n",
      "Trainable parameters: 530,442\n",
      "Frozen parameters: 151,277,314\n"
     ]
    }
   ],
   "source": [
    "# Initialize CLIP with MLP model\n",
    "model = CLIPWithMLP(\n",
    "    clip_model_name=clip_model_name,\n",
    "    num_classes=num_classes,\n",
    "    mlp_hidden_dim=mlp_hidden_dim,\n",
    "    mlp_dropout_rate=mlp_dropout_rate,\n",
    "    mlp_num_layers=mlp_num_layers,\n",
    "    freeze_clip=True,  # Only train the MLP head\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "# Count trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {total_params - trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/1250 [00:00<?, ?it/s]\n",
      "Epoch 1/10:   0%|          | 0/1250 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/DAP/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/DAP/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/DAP/BayesVLM/notebooks/clip_mlp.py:127\u001b[0m, in \u001b[0;36mCLIPWithMLP.forward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through CLIP + MLP\"\"\"\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Get image features from CLIP\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Pass through MLP head\u001b[39;00m\n\u001b[1;32m    130\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_head(image_features)\n",
      "File \u001b[0;32m~/Desktop/DAP/BayesVLM/notebooks/clip_mlp.py:118\u001b[0m, in \u001b[0;36mCLIPWithMLP.encode_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    117\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Encode images using CLIP visual encoder\"\"\"\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/DAP/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/DAP/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/DAP/BayesVLM/notebooks/../bayesvlm/vlm.py:365\u001b[0m, in \u001b[0;36mCLIPImageEncoder.forward\u001b[0;34m(self, batch, return_activations)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, return_activations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 365\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    366\u001b[0m     image_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(pixel_values\u001b[38;5;241m=\u001b[39mimages\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[1;32m    367\u001b[0m     image_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_encoder(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mimage_input)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 4"
     ]
    }
   ],
   "source": [
    "# Initialize optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(dm.train_dataloader(), desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Extract images and labels from the batch dictionary\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['class_id'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{loss.item():.4f}',\n",
    "            'Acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dm.train_dataloader())\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: Loss = {avg_loss:.4f}, Accuracy = {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(train_losses)\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(train_accuracies)\n",
    "ax2.set_title('Training Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with Uncertainty Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_uncertainty(model, dataloader, device, n_samples=100):\n",
    "    \"\"\"Evaluate model with uncertainty quantification\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_uncertainties_epistemic = []\n",
    "    all_uncertainties_aleatoric = []\n",
    "    all_labels = []\n",
    "    all_deterministic_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Evaluating'):\n",
    "            # Extract images and labels from the batch dictionary\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['class_id'].to(device)\n",
    "            \n",
    "            # Deterministic prediction (no dropout)\n",
    "            model.eval()\n",
    "            det_logits = model(images)\n",
    "            det_probs = F.softmax(det_logits, dim=-1)\n",
    "            \n",
    "            # Probabilistic prediction with uncertainty\n",
    "            mean_probs, epistemic_unc, aleatoric_unc = model.predict_with_uncertainty(\n",
    "                images, n_samples=n_samples\n",
    "            )\n",
    "            \n",
    "            all_predictions.append(mean_probs.cpu())\n",
    "            all_uncertainties_epistemic.append(epistemic_unc.cpu())\n",
    "            all_uncertainties_aleatoric.append(aleatoric_unc.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_deterministic_preds.append(det_probs.cpu())\n",
    "    \n",
    "    # Concatenate all results\n",
    "    predictions = torch.cat(all_predictions, dim=0)\n",
    "    uncertainties_epistemic = torch.cat(all_uncertainties_epistemic, dim=0)\n",
    "    uncertainties_aleatoric = torch.cat(all_uncertainties_aleatoric, dim=0)\n",
    "    labels = torch.cat(all_labels, dim=0)\n",
    "    det_predictions = torch.cat(all_deterministic_preds, dim=0)\n",
    "    \n",
    "    return predictions, uncertainties_epistemic, uncertainties_aleatoric, labels, det_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "predictions, epistemic_unc, aleatoric_unc, labels, det_predictions = evaluate_with_uncertainty(\n",
    "    model, dm.test_dataloader(), device, n_samples=n_uncertainty_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "def compute_metrics(predictions, labels, num_classes):\n",
    "    \"\"\"Compute accuracy, ECE, and NLPD\"\"\"\n",
    "    # Accuracy\n",
    "    predicted_classes = predictions.argmax(dim=-1)\n",
    "    accuracy = (predicted_classes == labels).float().mean().item()\n",
    "    \n",
    "    # ECE\n",
    "    ece_metric = MulticlassCalibrationError(num_classes=num_classes, n_bins=20, norm='l1')\n",
    "    ece = ece_metric(predictions, labels).item()\n",
    "    \n",
    "    # NLPD (Negative Log Predictive Density)\n",
    "    log_probs = torch.log(predictions + 1e-8)\n",
    "    nlpd = -log_probs.gather(1, labels.unsqueeze(1)).squeeze().mean().item()\n",
    "    \n",
    "    return accuracy, ece, nlpd\n",
    "\n",
    "# Compute metrics for both approaches\n",
    "acc_dropout, ece_dropout, nlpd_dropout = compute_metrics(predictions, labels, num_classes)\n",
    "acc_det, ece_det, nlpd_det = compute_metrics(det_predictions, labels, num_classes)\n",
    "\n",
    "print(\"\\n=== Results ===\")\n",
    "data = [\n",
    "    [\"Accuracy (↑)\", f\"{acc_dropout:.4f}\", f\"{acc_det:.4f}\"],\n",
    "    [\"ECE (↓)\", f\"{ece_dropout:.4f}\", f\"{ece_det:.4f}\"],\n",
    "    [\"NLPD (↓)\", f\"{nlpd_dropout:.4f}\", f\"{nlpd_det:.4f}\"]\n",
    "]\n",
    "\n",
    "print(tabulate(data, headers=[\"Metric\", \"Dropout (Bayesian)\", \"Deterministic\"], tablefmt=\"simple\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze uncertainty\n",
    "predicted_classes = predictions.argmax(dim=-1)\n",
    "correct_predictions = (predicted_classes == labels)\n",
    "\n",
    "# Uncertainty statistics\n",
    "epistemic_correct = epistemic_unc[correct_predictions]\n",
    "epistemic_incorrect = epistemic_unc[~correct_predictions]\n",
    "aleatoric_correct = aleatoric_unc[correct_predictions]\n",
    "aleatoric_incorrect = aleatoric_unc[~correct_predictions]\n",
    "\n",
    "print(f\"\\n=== Uncertainty Analysis ===\")\n",
    "print(f\"Epistemic Uncertainty:\")\n",
    "print(f\"  Correct predictions: {epistemic_correct.mean():.4f} ± {epistemic_correct.std():.4f}\")\n",
    "print(f\"  Incorrect predictions: {epistemic_incorrect.mean():.4f} ± {epistemic_incorrect.std():.4f}\")\n",
    "print(f\"\\nAleatoric Uncertainty:\")\n",
    "print(f\"  Correct predictions: {aleatoric_correct.mean():.4f} ± {aleatoric_correct.std():.4f}\")\n",
    "print(f\"  Incorrect predictions: {aleatoric_incorrect.mean():.4f} ± {aleatoric_incorrect.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Uncertainty Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot uncertainty distributions\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Epistemic uncertainty\n",
    "ax1.hist(epistemic_correct.numpy(), bins=50, alpha=0.7, label='Correct', density=True)\n",
    "ax1.hist(epistemic_incorrect.numpy(), bins=50, alpha=0.7, label='Incorrect', density=True)\n",
    "ax1.set_title('Epistemic Uncertainty Distribution')\n",
    "ax1.set_xlabel('Epistemic Uncertainty')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Aleatoric uncertainty\n",
    "ax2.hist(aleatoric_correct.numpy(), bins=50, alpha=0.7, label='Correct', density=True)\n",
    "ax2.hist(aleatoric_incorrect.numpy(), bins=50, alpha=0.7, label='Incorrect', density=True)\n",
    "ax2.set_title('Aleatoric Uncertainty Distribution')\n",
    "ax2.set_xlabel('Aleatoric Uncertainty')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Uncertainty scatter plot\n",
    "colors = ['green' if c else 'red' for c in correct_predictions]\n",
    "ax3.scatter(epistemic_unc.numpy(), aleatoric_unc.numpy(), c=colors, alpha=0.5, s=1)\n",
    "ax3.set_xlabel('Epistemic Uncertainty')\n",
    "ax3.set_ylabel('Aleatoric Uncertainty')\n",
    "ax3.set_title('Epistemic vs Aleatoric Uncertainty')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Confidence vs accuracy\n",
    "confidence = predictions.max(dim=-1)[0]\n",
    "ax4.scatter(confidence.numpy(), correct_predictions.float().numpy(), alpha=0.5, s=1)\n",
    "ax4.set_xlabel('Prediction Confidence')\n",
    "ax4.set_ylabel('Correct (1) / Incorrect (0)')\n",
    "ax4.set_title('Confidence vs Correctness')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Predictions with Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some examples with high/low uncertainty\n",
    "# Get a small subset for visualization\n",
    "n_samples = 8\n",
    "batch = next(iter(dm.test_dataloader()))\n",
    "test_images = batch['image'][:n_samples].to(device)\n",
    "test_labels = batch['class_id'][:n_samples]\n",
    "\n",
    "# Get predictions and uncertainties\n",
    "with torch.no_grad():\n",
    "    sample_preds, sample_epistemic, sample_aleatoric = model.predict_with_uncertainty(\n",
    "        test_images, n_samples=n_uncertainty_samples\n",
    "    )\n",
    "\n",
    "# Convert to numpy for plotting\n",
    "test_images_np = test_images.cpu().numpy()\n",
    "sample_preds_np = sample_preds.cpu().numpy()\n",
    "sample_epistemic_np = sample_epistemic.cpu().numpy()\n",
    "sample_aleatoric_np = sample_aleatoric.cpu().numpy()\n",
    "test_labels_np = test_labels.cpu().numpy()\n",
    "\n",
    "# Plot examples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Normalize image for display\n",
    "    img = test_images_np[i].transpose(1, 2, 0)\n",
    "    img = (img - img.min()) / (img.max() - img.min())\n",
    "    \n",
    "    predicted_class = sample_preds_np[i].argmax()\n",
    "    confidence = sample_preds_np[i].max()\n",
    "    true_class = test_labels_np[i]\n",
    "    \n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(\n",
    "        f'True: {true_class}, Pred: {predicted_class}\\n'\n",
    "        f'Conf: {confidence:.3f}\\n'\n",
    "        f'Epist: {sample_epistemic_np[i]:.3f}\\n'\n",
    "        f'Aleat: {sample_aleatoric_np[i]:.3f}'\n",
    "    )\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Model Architecture**: Added a trainable MLP head to a frozen CLIP model\n",
    "2. **Training**: Trained only the MLP parameters while keeping CLIP frozen\n",
    "3. **Uncertainty Quantification**: Used dropout to estimate both epistemic and aleatoric uncertainty\n",
    "4. **Evaluation**: Compared deterministic vs probabilistic predictions\n",
    "5. **Analysis**: Visualized uncertainty distributions and their correlation with prediction correctness\n",
    "\n",
    "Key observations:\n",
    "- The Bayesian (dropout) approach typically provides better calibration (lower ECE)\n",
    "- Higher uncertainty is generally associated with incorrect predictions\n",
    "- Epistemic uncertainty captures model uncertainty, while aleatoric captures data uncertainty\n",
    "- The approach allows for uncertainty-aware decision making in downstream applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
